# ═══════════════════════════════════════════════════════════════════════════════
# Ollama Chat - Backend
# ═══════════════════════════════════════════════════════════════════════════════

OLLAMA_URL = "http://localhost:11434"

# ─────────────────────────────────────────────────────────────────────────────
# Ollama API
# ─────────────────────────────────────────────────────────────────────────────

# Prüft ob Ollama Server läuft
fn check_ollama(args):
    response = http_get(OLLAMA_URL + "/api/tags")
    return response["status"] == 200

# Holt Liste aller verfügbaren Modelle
fn list_models(args):
    response = http_get(OLLAMA_URL + "/api/tags")
    
    if response["status"] != 200:
        return []
    
    body = json_parse(response["body"])
    
    models = []
    if "models" in body:
        for model in body["models"]:
            if "name" in model:
                models = models + [model["name"]]
    
    return models

# ─────────────────────────────────────────────────────────────────────────────
# Streaming Chat
# ─────────────────────────────────────────────────────────────────────────────

# Startet einen Streaming-Chat
fn chat_stream_start(args):
    model = args["model"]
    messages = args["messages"]
    
    temperature = 0.7
    if "temperature" in args:
        temperature = args["temperature"]
    
    request_body = {
        "model": model,
        "messages": messages,
        "stream": true,
        "options": {
            "temperature": temperature
        }
    }
    
    session_id = http_stream_start(OLLAMA_URL + "/api/chat", request_body)
    return {"session_id": session_id}

# Pollt Streaming-Chunks
fn chat_stream_poll(args):
    session_id = args["session_id"]
    
    result = http_stream_poll(session_id)
    
    # Parse NDJSON chunks
    content = ""
    thinking = ""
    done = false
    
    for chunk_str in result["chunks"]:
        chunk = json_parse(chunk_str)
        
        if "message" in chunk:
            msg = chunk["message"]
            # Content für finale Antwort
            if "content" in msg:
                if msg["content"] != "":
                    content = content + msg["content"]
            # Thinking für Reasoning
            if "thinking" in msg:
                if msg["thinking"] != "":
                    thinking = thinking + msg["thinking"]
        
        if "done" in chunk:
            if chunk["done"] == true:
                done = true
    
    return {
        "content": content,
        "thinking": thinking,
        "done": done or result["done"],
        "error": result["error"]
    }

# Schließt eine Stream-Session
fn chat_stream_close(args):
    session_id = args["session_id"]
    return http_stream_close(session_id)

# ─────────────────────────────────────────────────────────────────────────────
# Non-Streaming Chat (Fallback)
# ─────────────────────────────────────────────────────────────────────────────

fn chat(args):
    model = args["model"]
    messages = args["messages"]
    
    temperature = 0.7
    if "temperature" in args:
        temperature = args["temperature"]
    
    request_body = {
        "model": model,
        "messages": messages,
        "stream": false,
        "options": {
            "temperature": temperature
        }
    }
    
    response = http_post_json(OLLAMA_URL + "/api/chat", request_body)
    
    if response["status"] != 200:
        return {"error": "Ollama error: " + str(response["status"])}
    
    body = response["body"]
    
    result = {
        "content": "",
        "thinking": none,
        "model": model
    }
    
    if "message" in body:
        msg = body["message"]
        if "content" in msg:
            result["content"] = msg["content"]
        if "thinking" in msg:
            result["thinking"] = msg["thinking"]
    
    return result
